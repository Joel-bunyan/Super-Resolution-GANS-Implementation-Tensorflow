{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWIdulUheFeQ"
      },
      "outputs": [],
      "source": [
        "from numpy import array\n",
        "from scipy.misc import imresize\n",
        "def hr_images(images):\n",
        "    images_hr = array(images)\n",
        "    return images_hr\n",
        "\n",
        "def lr_images(images_real , downscale):\n",
        "    images = []\n",
        "    for img in  range(len(images_real)):\n",
        "        images.append(imresize(images_real[img], [images_real[img].shape[0]//downscale, images_real[img].shape[1]//downscale], interp='bicubic', mode=None))\n",
        "    images_lr = array(images)\n",
        "    return images_lr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.core import Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import UpSampling2D\n",
        "from keras.layers import Input\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.models import Model\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "from keras.layers import add\n",
        "\n",
        "def res_block_gen(model, kernal_size, filters, strides):\n",
        "    gen = model\n",
        "\n",
        "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
        "    model = BatchNormalization(momentum = 0.5)(model)\n",
        "    model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n",
        "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
        "    model = BatchNormalization(momentum = 0.5)(model)\n",
        "\n",
        "    model = add([gen, model])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def up_sampling_block(model, kernal_size, filters, strides):\n",
        "\n",
        "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
        "    model = UpSampling2D(size = 2)(model)\n",
        "    model = LeakyReLU(alpha = 0.2)(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "class Generator(object):\n",
        "    def __init__(self, noise_shape):\n",
        "\n",
        "        self.noise_shape = noise_shape\n",
        "    def generator(self):\n",
        "\n",
        "        gen_input = Input(shape = self.noise_shape)\n",
        "\n",
        "        model = Conv2D(filters = 64, kernel_size = 9, strides = 1, padding = \"same\")(gen_input)\n",
        "        model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n",
        "\n",
        "        gen_model = model\n",
        "\n",
        "        for index in range(16):\n",
        "            model = res_block_gen(model, 3, 64, 1)\n",
        "\n",
        "        model = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(model)\n",
        "        model = BatchNormalization(momentum = 0.5)(model)\n",
        "        model = add([gen_model, model])\n",
        "\n",
        "        for index in range(2):\n",
        "            model = up_sampling_block(model, 3, 256, 1)\n",
        "\n",
        "        model = Conv2D(filters = 3, kernel_size = 9, strides = 1, padding = \"same\")(model)\n",
        "        model = Activation('tanh')(model)\n",
        "\n",
        "        generator_model = Model(inputs = gen_input, outputs = model)\n",
        "        return generator_model"
      ],
      "metadata": {
        "id": "TkJnsQtyjJEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.models import Model\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "\n",
        "def discriminator_block(model, filters, kernel_size, strides):\n",
        "\n",
        "    model = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n",
        "    model = BatchNormalization(momentum = 0.5)(model)\n",
        "    model = LeakyReLU(alpha = 0.2)(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "class Discriminator(object):\n",
        "\n",
        "    def __init__(self, image_shape):\n",
        "\n",
        "        self.image_shape = image_shape\n",
        "\n",
        "    def discriminator(self):\n",
        "\n",
        "        dis_input = Input(shape = self.image_shape)\n",
        "\n",
        "        model = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(dis_input)\n",
        "        model = LeakyReLU(alpha = 0.2)(model)\n",
        "\n",
        "        model = discriminator_block(model, 64, 3, 2)\n",
        "        model = discriminator_block(model, 128, 3, 1)\n",
        "        model = discriminator_block(model, 128, 3, 2)\n",
        "        model = discriminator_block(model, 256, 3, 1)\n",
        "        model = discriminator_block(model, 256, 3, 2)\n",
        "        model = discriminator_block(model, 512, 3, 1)\n",
        "        model = discriminator_block(model, 512, 3, 2)\n",
        "\n",
        "        model = Flatten()(model)\n",
        "        model = Dense(1024)(model)\n",
        "        model = LeakyReLU(alpha = 0.2)(model)\n",
        "\n",
        "        model = Dense(1)(model)\n",
        "        model = Activation('sigmoid')(model)\n",
        "\n",
        "        discriminator_model = Model(inputs = dis_input, outputs = model)\n",
        "\n",
        "        return discriminator_model"
      ],
      "metadata": {
        "id": "QqOHG5udjo9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "def vgg_loss(self, y_true, y_pred):\n",
        "    vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=self.image_shape)\n",
        "    vgg19.trainable = False\n",
        "    for l in vgg19.layers:\n",
        "        l.trainable = False\n",
        "    model = Model(inputs=vgg19.input, outputs=vgg19.get_layer('block5_conv4').output)\n",
        "    model.trainable = False\n",
        "\n",
        "    return K.mean(K.square(model(y_true) - model(y_pred)))"
      ],
      "metadata": {
        "id": "Qtez9zi2j4Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "\n",
        "def get_gan_network(discriminator, shape, generator, optimizer, vgg_loss):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(shape=shape)\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = Model(inputs=gan_input, outputs=[x,gan_output])\n",
        "    gan.compile(loss=[vgg_loss, \"binary_crossentropy\",\"binary_crossentropy\"], loss_weights=[1., 1e-3], optimizer=optimizer)\n",
        "\n",
        "    return gan"
      ],
      "metadata": {
        "id": "2mMv0AjXj7yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer():\n",
        "\n",
        "    adam = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    return adam"
      ],
      "metadata": {
        "id": "5HGjfWfkj8Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "downscale_factor = 4\n",
        "image_shape = (384,384,3)\n",
        "\n",
        "def train(epochs, batch_size, input_dir, output_dir, model_save_dir, number_of_images, train_test_ratio):\n",
        "    x_train_lr, x_train_hr, x_test_lr, x_test_hr = load_training_data(input_dir, '.jpg', number_of_images, train_test_ratio)\n",
        "\n",
        "    batch_count = int(x_train_hr.shape[0] / batch_size)\n",
        "    shape = (image_shape[0]//downscale_factor, image_shape[1]//downscale_factor, image_shape[2])\n",
        "\n",
        "    generator = Generator(shape).generator()\n",
        "    discriminator = Discriminator(image_shape).discriminator()\n",
        "\n",
        "    optimizer = get_optimizer()\n",
        "    generator.compile(loss=vgg_loss, optimizer=optimizer)\n",
        "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
        "\n",
        "    gan = get_gan_network(discriminator, shape, generator, optimizer, vgg_loss)\n",
        "\n",
        "    loss_file = open(model_save_dir + 'losses.txt' , 'w+')\n",
        "    loss_file.close()\n",
        "\n",
        "    for e in range(1, epochs+1):\n",
        "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in tqdm(range(batch_count)):\n",
        "\n",
        "            rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
        "\n",
        "            image_batch_hr = x_train_hr[rand_nums]\n",
        "            image_batch_lr = x_train_lr[rand_nums]\n",
        "            generated_images_sr = generator.predict(image_batch_lr)\n",
        "\n",
        "            real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
        "            fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
        "\n",
        "            discriminator.trainable = True\n",
        "\n",
        "            d_loss_real = discriminator.train_on_batch(image_batch_hr, real_data_Y)\n",
        "            d_loss_fake = discriminator.train_on_batch(generated_images_sr, fake_data_Y)\n",
        "            discriminator_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "\n",
        "            rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
        "            image_batch_hr = x_train_hr[rand_nums]\n",
        "            image_batch_lr = x_train_lr[rand_nums]\n",
        "\n",
        "            gan_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
        "            discriminator.trainable = False\n",
        "            gan_loss = gan.train_on_batch(image_batch_lr, [image_batch_hr,gan_Y])\n",
        "\n",
        "            gan_Y_gen = np.ones(batch_size) - np.random.random_sample(batch_size) * 0.2\n",
        "            generator_loss = gan.train_on_batch(image_batch_lr, [image_batch_hr, gan_Y, gan_Y_gen])\n",
        "\n",
        "        print(\"discriminator_loss : %f\" % discriminator_loss)\n",
        "        print(\"gan_loss :\", gan_loss)\n",
        "        gan_loss = str(gan_loss)\n",
        "\n",
        "        loss_file = open(model_save_dir + 'losses.txt' , 'a')\n",
        "        loss_file.write('epoch%d : gan_loss = %s ; discriminator_loss = %f\\n' %(e, gan_loss, discriminator_loss) )\n",
        "        loss_file.close()\n",
        "\n",
        "        if e == 1 or e % 10 == 0:\n",
        "            Utils.plot_generated_images(output_dir, e, generator, x_test_hr, x_test_lr)\n",
        "        if e % 500 == 0:\n",
        "            generator.save(model_save_dir + 'gen_model%d.h5' % e)\n",
        "            discriminator.save(model_save_dir + 'dis_model%d.h5' % e)"
      ],
      "metadata": {
        "id": "4WTUi92xj_f-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}